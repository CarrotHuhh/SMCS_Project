Data Collection & Sources

We will rely on public and operational datasets to build route–hour level cancellation risk estimates. The primary source is the NDOV NeTEx feed (https://data.ndovloket.nl). NeTEx (Network Timetable Exchange) is an XML-based standard that encodes schedules, service patterns, calendar rules, journey patterns, and stop sequences across operators; it is therefore the authoritative source for scheduled trips and their temporal structure. We will parse NeTEx to extract scheduled trips, timetable identifiers, service dates, route and journey-pattern definitions, and stop sequences, allowing precise mapping from scheduled journeys to route×hour bins. Because NeTEx can be verbose and complex, we plan to leverage existing NeTEx→GTFS converters where possible or implement a focused parser (using `lxml`/`xml.etree`) to produce tabular schedules compatible with Pandas and downstream tooling. Supplementary datasets include the `haltes` file for precise stop locations and stop-to-route mappings, and the `bezetting` (occupancy) dataset for vehicle crowding measures that can serve as covariates related to operational strain. 

For example, the `bezetting/qbuzz` dataset provides daily records covering roughly the past six months, enabling near-term historical analysis and straightforward construction of route–hour labels; in contrast, the `NeTEx/qbuzz` folder contains weekly NeTEx extracts covering approximately the most recent four months. These differing update cadences will inform our ingestion frequency, windowing choices, and validation strategy.

When available, we will also incorporate GTFS-RT or AVL logs to obtain observed trip records, delays, and cancellations for labeling and validation. Aggregation and preprocessing will convert raw schedules and feeds into route×hour tables containing scheduled trip counts, observed trip counts, cancellations, delay summaries, occupancy statistics, and temporal covariates (day-of-week, hour-of-day). We will store intermediate artifacts as CSV or Parquet and maintain a compact dataset (Parquet or SQLite) keyed by route, date, and hour for reproducible experiments.